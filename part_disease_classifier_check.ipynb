{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y데이터 준비에 문제가 없음에도 계속 손실함수 관련 문제가 발생한다면, 데이터 준비를 위해 내가 작성한 함수에 어떤 문제가 있을 수도 있음\n",
    "# 이를 확인하기 위해, 내가 작성한 함수를 사용하지 않고 다시 x, y dataset, dataloader을 준비해서 학습을 시도해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PartDiseaseClassifier model build\n",
    "\n",
    "class PartDiseaseClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(PartDiseaseClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.layer2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10 = pd.read_csv('./data_preprocessed/label/train_10_cleaned.csv', encoding='utf-8')\n",
    "test_data = pd.read_csv('./data_preprocessed/label/test_cleaned.csv', encoding='utf-8')\n",
    "\n",
    "df_train = train_10.copy()\n",
    "df_test = test_data.copy()\n",
    "\n",
    "train_features_npz = np.load('./data_preprocessed/part_image_features/tr_10_cleaned.npz')\n",
    "test_features_npz = np.load('./data_preprocessed/part_image_features/vl_cleaned.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_50 = pd.read_csv('./data_preprocessed/label/train_50_cleaned.csv', encoding='utf-8')\n",
    "test_data = pd.read_csv('./data_preprocessed/label/test_cleaned.csv', encoding='utf-8')\n",
    "\n",
    "df_train_50 = train_50.copy()\n",
    "df_test = test_data.copy()\n",
    "\n",
    "train_features_50_npz = np.load('./data_preprocessed/part_image_features/tr_50_cleaned.npz')\n",
    "test_features_npz = np.load('./data_preprocessed/part_image_features/vl_cleaned.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['features'] = None\n",
    "train_data = df_train[['class', 'tr_id', 'disease_status', 'features']].copy()\n",
    "train_data = train_data[train_data['disease_status'] == 'Y'].reset_index(drop=True)\n",
    "\n",
    "df_test['features'] = None\n",
    "test_set = df_test[['class', 'vl_id', 'disease_status', 'features']].copy()\n",
    "test_set = test_set[test_set['disease_status'] == 'Y'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list_tr = list(train_features_npz.keys())\n",
    "key_list_vl = list(test_features_npz.keys())\n",
    "\n",
    "for idx, row in train_data.iterrows():\n",
    "    id = row['tr_id']\n",
    "\n",
    "    if id in key_list_tr:\n",
    "        train_data.at[idx, 'features'] = train_features_npz[id]\n",
    "\n",
    "for idx, row in test_set.iterrows():\n",
    "    id = row['vl_id']\n",
    "\n",
    "    if id in key_list_vl:\n",
    "        test_set.at[idx, 'features'] = test_features_npz[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data['class_label'] = None\n",
    "\n",
    "for idx, row in train_data.iterrows():\n",
    "    origin = row['class']\n",
    "    if origin == 'blight':\n",
    "        train_data.at[idx, 'class_label'] = 0\n",
    "\n",
    "    elif origin == 'wilt':\n",
    "        train_data.at[idx, 'class_label'] = 1\n",
    "    \n",
    "    elif origin == 'scorch':\n",
    "        train_data.at[idx, 'class_label'] = 2\n",
    "\n",
    "    else:\n",
    "        train_data.at[idx, 'class_label'] = 3\n",
    "\n",
    "test_set['class_label'] = None\n",
    "\n",
    "for idx, row in test_set.iterrows():\n",
    "    origin = row['class']\n",
    "    if origin == 'blight':\n",
    "        test_set.at[idx, 'class_label'] = 0\n",
    "\n",
    "    elif origin == 'wilt':\n",
    "        test_set.at[idx, 'class_label'] = 1\n",
    "    \n",
    "    elif origin == 'scorch':\n",
    "        test_set.at[idx, 'class_label'] = 2\n",
    "\n",
    "    else:\n",
    "        test_set.at[idx, 'class_label'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(train_data['features'], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data['class_label'], dtype=torch.long)\n",
    "x_test = torch.tensor(test_set['features'], dtype=torch.float32)\n",
    "y_test = torch.tensor(test_set['class_label'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4466, 1, 1000])\n",
      "torch.Size([1800, 1, 1000])\n",
      "torch.Size([4466])\n",
      "torch.Size([1800])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4466, 1000])\n",
      "torch.Size([1800, 1000])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.squeeze(x_train, 1)\n",
    "x_test = torch.squeeze(x_test, 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_y_for_model(df, features_npz, id_col, batch_size, shuffle_option):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['features'] = None\n",
    "    df_copy['class_label'] = None\n",
    "    df_copy = df_copy[df_copy['disease_status'] == 'Y'].reset_index(drop=True)\n",
    "    features_keys = list(features_npz.keys())\n",
    "\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        id = row[id_col]\n",
    "        if id in features_keys:\n",
    "            df_copy.at[idx, 'features'] = features_npz[id]\n",
    "\n",
    "    for index, row in df_copy.iterrows():\n",
    "        origin = row['class']\n",
    "        if origin == 'blight':\n",
    "            df_copy.at[index, 'class_label'] = 0\n",
    "        elif origin == 'wilt':\n",
    "            df_copy.at[index, 'class_label'] = 1  \n",
    "        elif origin == 'scorch':\n",
    "            df_copy.at[index, 'class_label'] = 2\n",
    "        else:\n",
    "            df_copy.at[index, 'class_label'] = 3\n",
    "\n",
    "    features_array = np.array(df_copy['features'].tolist())\n",
    "    x_data = torch.tensor(features_array, dtype=torch.float32)\n",
    "    y_data = torch.tensor(df_copy['class_label'], dtype=torch.long)\n",
    "    x_data = torch.squeeze(x_data, 1)\n",
    "\n",
    "    dataset = TensorDataset(x_data, y_data)\n",
    "    dataset_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle_option)\n",
    "\n",
    "    return dataset_dataloader, df_copy, x_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, df_train, x_train = status_y_for_model(df=df_train, features_npz=train_features_npz, id_col='tr_id', batch_size=32, shuffle_option=True)\n",
    "val_loader, df_test, x_test = status_y_for_model(df=df_test, features_npz=test_features_npz, id_col='vl_id', batch_size=32, shuffle_option=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, df_train_50, x_train_50 = status_y_for_model(df=df_train_50, features_npz=train_features_50_npz, id_col='tr_id', batch_size=32, shuffle_option=True)\n",
    "val_loader, df_test, x_test = status_y_for_model(df=df_test, features_npz=test_features_npz, id_col='vl_id', batch_size=32, shuffle_option=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartDiseaseClassifier(\n",
       "  (layer1): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PartDiseaseClassifier(input_size=x_train.shape[1], num_classes=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartDiseaseClassifier(\n",
       "  (layer1): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (layer2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PartDiseaseClassifier(input_size=x_train_50.shape[1], num_classes=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6923336114628902, Accuracy: 0.6922029281579843\n",
      "Validation Loss: 0.5534694317662925, Validation Accuracy: 0.7527777777777778\n",
      "Epoch 2/30, Loss: 0.576758209072167, Accuracy: 0.7430711610486891\n",
      "Validation Loss: 0.5294938045635558, Validation Accuracy: 0.7577777777777778\n",
      "Epoch 3/30, Loss: 0.5312818874785583, Accuracy: 0.7641130405175349\n",
      "Validation Loss: 0.5034604754887129, Validation Accuracy: 0.7716666666666666\n",
      "Epoch 4/30, Loss: 0.49819582752672414, Accuracy: 0.7814776983316309\n",
      "Validation Loss: 0.5047111715141096, Validation Accuracy: 0.7722222222222223\n",
      "Epoch 5/30, Loss: 0.4648050108116956, Accuracy: 0.7995914198161389\n",
      "Validation Loss: 0.5127169531688356, Validation Accuracy: 0.7727777777777778\n",
      "Epoch 6/30, Loss: 0.44898226285811865, Accuracy: 0.8042900919305414\n",
      "Validation Loss: 0.5274534949608016, Validation Accuracy: 0.77\n",
      "Epoch 7/30, Loss: 0.4282626730099742, Accuracy: 0.8179094313925774\n",
      "Validation Loss: 0.48368827135939346, Validation Accuracy: 0.7944444444444444\n",
      "Epoch 8/30, Loss: 0.4048108658499707, Accuracy: 0.823425263874702\n",
      "Validation Loss: 0.5195705242324293, Validation Accuracy: 0.7788888888888889\n",
      "Epoch 9/30, Loss: 0.39389295997767665, Accuracy: 0.8337078651685393\n",
      "Validation Loss: 0.5188135895551297, Validation Accuracy: 0.7916666666666666\n",
      "Epoch 10/30, Loss: 0.3701039466820251, Accuracy: 0.8394960844399046\n",
      "Validation Loss: 0.4916225593341024, Validation Accuracy: 0.7988888888888889\n",
      "Epoch 11/30, Loss: 0.36428909122229663, Accuracy: 0.8452162070139598\n",
      "Validation Loss: 0.5025072399722902, Validation Accuracy: 0.8077777777777778\n",
      "Epoch 12/30, Loss: 0.34978577820888535, Accuracy: 0.8486891385767791\n",
      "Validation Loss: 0.4880152182620868, Validation Accuracy: 0.8088888888888889\n",
      "Epoch 13/30, Loss: 0.33815637004433896, Accuracy: 0.8572693224378618\n",
      "Validation Loss: 0.5078587346432502, Validation Accuracy: 0.8027777777777778\n",
      "Epoch 14/30, Loss: 0.3211545695536537, Accuracy: 0.8655771195097037\n",
      "Validation Loss: 0.5044741624018603, Validation Accuracy: 0.8138888888888889\n",
      "Epoch 15/30, Loss: 0.3173062910197997, Accuracy: 0.8682328907048008\n",
      "Validation Loss: 0.5265007643846044, Validation Accuracy: 0.8094444444444444\n",
      "Epoch 16/30, Loss: 0.3016874493791647, Accuracy: 0.8733401430030644\n",
      "Validation Loss: 0.5225592238599794, Validation Accuracy: 0.8105555555555556\n",
      "Epoch 17/30, Loss: 0.29407946106395433, Accuracy: 0.8764044943820225\n",
      "Validation Loss: 0.5313507650505033, Validation Accuracy: 0.8016666666666666\n",
      "Epoch 18/30, Loss: 0.2985757704101876, Accuracy: 0.8785154919986381\n",
      "Validation Loss: 0.5181071278557443, Validation Accuracy: 0.8094444444444444\n",
      "Epoch 19/30, Loss: 0.28539929999959757, Accuracy: 0.8810350697991147\n",
      "Validation Loss: 0.5925743412553218, Validation Accuracy: 0.8055555555555556\n",
      "Epoch 20/30, Loss: 0.27100716434695105, Accuracy: 0.8908409942117808\n",
      "Validation Loss: 0.5809748656394189, Validation Accuracy: 0.8105555555555556\n",
      "Epoch 21/30, Loss: 0.27112708672524016, Accuracy: 0.8888661899897855\n",
      "Validation Loss: 0.5231199076301173, Validation Accuracy: 0.8188888888888889\n",
      "Epoch 22/30, Loss: 0.266160462855124, Accuracy: 0.889410963568267\n",
      "Validation Loss: 0.569205667888909, Validation Accuracy: 0.8127777777777778\n",
      "Epoch 23/30, Loss: 0.24134133765088447, Accuracy: 0.9002383384405857\n",
      "Validation Loss: 0.5673799483399642, Validation Accuracy: 0.8116666666666666\n",
      "Epoch 24/30, Loss: 0.25202118268557105, Accuracy: 0.8982635342185904\n",
      "Validation Loss: 0.5727435536029046, Validation Accuracy: 0.8233333333333334\n",
      "Epoch 25/30, Loss: 0.24264045876972296, Accuracy: 0.8989445011916922\n",
      "Validation Loss: 0.5859680820191115, Validation Accuracy: 0.82\n",
      "Epoch 26/30, Loss: 0.23139443774433696, Accuracy: 0.9060946544092612\n",
      "Validation Loss: 0.6146600943124085, Validation Accuracy: 0.815\n",
      "Epoch 27/30, Loss: 0.24397650496599982, Accuracy: 0.902281239359891\n",
      "Validation Loss: 0.5685862040049151, Validation Accuracy: 0.8177777777777778\n",
      "Epoch 28/30, Loss: 0.23198081561098433, Accuracy: 0.9062308478038815\n",
      "Validation Loss: 0.6047036544534198, Validation Accuracy: 0.8183333333333334\n",
      "Epoch 29/30, Loss: 0.22351877354004285, Accuracy: 0.9089547156962887\n",
      "Validation Loss: 0.5945905380343136, Validation Accuracy: 0.8177777777777778\n",
      "Epoch 30/30, Loss: 0.22111677229891416, Accuracy: 0.9112019067075247\n",
      "Validation Loss: 0.594235185468406, Validation Accuracy: 0.8138888888888889\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {train_accuracy}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_accuracy = correct_preds / total_preds\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berry_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
